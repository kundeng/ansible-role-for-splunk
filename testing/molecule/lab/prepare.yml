---
# Lab Infrastructure Preparation
# Sets up SSH connectivity and basic infrastructure for all lab containers

- name: Setup Splunk Lab Infrastructure  
  hosts: all:!git_server:!jumpbox
  gather_facts: true
  tasks:
    - name: Wait for systemd to be ready
      wait_for:
        path: /run/systemd/system
      when: ansible_service_mgr == "systemd" and inventory_hostname != 'ansible-controller'

    - name: Relax PAM account checks on RedHat family (avoid pam_sepermit denials)
      block:
        - name: Check if pam_sepermit is present in sshd PAM stack
          command: awk '/^account\s+required\s+pam_sepermit\.so/ {print; exit 0} END {if (NR==0) exit 1}' /etc/pam.d/sshd
          register: pam_sepermit_present
          changed_when: false
          failed_when: false

        - name: Make pam_sepermit optional (do not hard-fail account phase)
          replace:
            path: /etc/pam.d/sshd
            regexp: '^account\s+required\s+pam_sepermit\.so'
            replace: 'account    optional     pam_sepermit.so'
          when: pam_sepermit_present.rc == 0
      when: ansible_os_family == 'RedHat' and inventory_hostname != 'ansible-controller'

    - name: Generate SSH keypair on molecule-runner (named volume)
      openssh_keypair:
        path: /shared/ssh_keys/id_rsa
        type: rsa
        size: 2048
        comment: "splunk-test-cluster"
        mode: '0600'
        force: true
      run_once: true
      delegate_to: localhost

    - name: Set up SSH infrastructure for all Splunk hosts
      block:
        - name: Ensure SSH server is running (already configured in Dockerfile)
          systemd:
            name: "{{ 'sshd' if ansible_os_family == 'RedHat' else 'ssh' }}"
            state: started
            enabled: true
          when: inventory_hostname != 'ansible-controller'

        # NOTE: ansible user and sudo already configured in Dockerfile base images
        # No need to recreate user or configure sudo - this was causing PAM conflicts

        - name: Create ansible user SSH directory
          file:
            path: /home/ansible/.ssh
            state: directory
            owner: ansible
            group: ansible
            mode: '0700'

    - name: Fetch SSH keys from named volume (run once)
      slurp:
        src: "{{ item }}"
      loop:
        - /shared/ssh_keys/id_rsa
        - /shared/ssh_keys/id_rsa.pub
      register: runner_keys
      run_once: true
      delegate_to: localhost

    - name: Extract runner key contents (run once)
      set_fact:
        runner_private_key: "{{ (runner_keys.results | selectattr('item','equalto','/shared/ssh_keys/id_rsa') | first).content | b64decode }}"
        runner_public_key: "{{ (runner_keys.results | selectattr('item','equalto','/shared/ssh_keys/id_rsa.pub') | first).content | b64decode }}"
      run_once: true

    - name: Set up SSH infrastructure for all Splunk hosts
      block:
        - name: Copy SSH public key to authorized_keys
          copy:
            content: "{{ runner_public_key }}"
            dest: /home/ansible/.ssh/authorized_keys
            owner: ansible
            group: ansible
            mode: '0600'

        - name: Configure SSH client settings
          blockinfile:
            path: /home/ansible/.ssh/config
            create: true
            owner: ansible
            group: ansible
            mode: '0600'
            block: |
              Host *
                StrictHostKeyChecking no
                UserKnownHostsFile /dev/null
                LogLevel ERROR

        - name: Create test data directories
          file:
            path: "{{ item }}"
            state: directory
            mode: '0755'
          loop:
            - /var/log/test-data
            - /tmp/test-logs
          when: inventory_hostname in groups['uf']

      when: inventory_hostname != 'ansible-controller'

    - name: Permit user logins on systemd hosts (clear pam_nologin gate)
      block:
        - name: Remove /run/nologin if present
          file:
            path: /run/nologin
            state: absent

        - name: Start systemd-user-sessions (opens user logins)
          systemd:
            name: systemd-user-sessions
            state: started
            daemon_reload: true

        - name: Enable systemd-user-sessions at boot (ignore if static)
          systemd:
            name: systemd-user-sessions
            enabled: true
          ignore_errors: true
      when: ansible_service_mgr == "systemd" and inventory_hostname != 'ansible-controller'

    - name: Configure Ansible Controller
      block:
        - name: Ensure apt cache updated (controller)
          apt:
            update_cache: true
          when: ansible_os_family == 'Debian'

        - name: Install Node.js and npm for wetty
          apt:
            name:
              - nodejs
              - npm
            state: present
          when: ansible_os_family == 'Debian'

        - name: Install wetty globally
          npm:
            name: wetty
            global: true
            state: present
        - name: Create workspace SSH directory
          file:
            path: /workspace/.ssh
            state: directory
            mode: '0700'
            
        - name: Copy SSH keys to ansible-controller workspace
          copy:
            content: "{{ item.content }}"
            dest: "{{ item.dest }}"
            mode: "{{ item.mode }}"
          loop:
            - { content: "{{ runner_private_key }}", dest: /workspace/.ssh/id_rsa, mode: '0600' }
            - { content: "{{ runner_public_key }}", dest: /workspace/.ssh/id_rsa.pub, mode: '0644' }
          
        - name: Create ansible controller SSH config
          copy:
            content: |
              Host *
                StrictHostKeyChecking no
                UserKnownHostsFile /dev/null
                LogLevel ERROR
            dest: /workspace/.ssh/config
            mode: '0600'

        - name: Create wetty systemd service
          copy:
            dest: /etc/systemd/system/wetty.service
            mode: '0644'
            content: |
              [Unit]
              Description=Wetty web terminal
              After=network.target ssh.service

              [Service]
              Type=simple
              ExecStart=/usr/local/bin/wetty --port 3000 --host 0.0.0.0 --base /wetty/ --command "/bin/bash --login" --working-dir /workspace
              Restart=on-failure
              RestartSec=5

              [Install]
              WantedBy=multi-user.target

        - name: Reload systemd and enable/start wetty
          systemd:
            name: wetty
            state: started
            enabled: true
            daemon_reload: true

        - name: Create ansible inventory for SSH testing
          copy:
            content: |
              [clustermanager]
              splunk-master ansible_host=splunk-master

              [licensemaster]
              splunk-license ansible_host=splunk-license

              [deploymentserver]
              splunk-fwdmanager ansible_host=splunk-fwdmanager

              [indexer]
              splunkapp-prod01 ansible_host=splunkapp-prod01
              splunkapp-prod02 ansible_host=splunkapp-prod02

              [shcluster] 
              splunkshc-prod01 ansible_host=splunkshc-prod01
              splunkshc-prod02 ansible_host=splunkshc-prod02

              [shdeployer]
              splunk-deploy ansible_host=splunk-deploy

              [uf]
              splunk-uf01 ansible_host=splunk-uf01

              [all:vars]
              ansible_user=ansible
              ansible_ssh_private_key_file=/workspace/.ssh/id_rsa
              ansible_ssh_common_args=-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null
            dest: /workspace/inventory.ini
            mode: '0644'

        # Wetty is now managed by systemd on the controller

      when: inventory_hostname == 'ansible-controller'

- name: Bootstrap Git Server
  hosts: git_server  
  gather_facts: false  # Git server doesn't need Python3/Ansible management
  connection: docker   # Use docker connection, not SSH
  tasks:
    - name: Check Gitea container is running (no Python3 needed)
      raw: "echo 'Git server container running - Gitea will auto-start'"
      
    - name: Create test repositories (placeholder)
      raw: "echo 'Gitea ready for app repository testing'"

- name: Bootstrap Remote.it Jumpbox
  hosts: jumpbox
  gather_facts: false  # Jumpbox doesn't need Ansible management  
  connection: docker   # Use docker connection, not SSH
  tasks:
    - name: Check if Remote.it registration code is set
      raw: "echo $R3_REGISTRATION_CODE"
      register: r3_code_check
      
    - name: Wait for Remote.it jumpbox initialization (only if registration code set)
      raw: "sleep 10 && echo 'Remote.it jumpbox initializing - check logs with: docker logs remoteit-jumpbox'"
      when: r3_code_check.stdout is defined and r3_code_check.stdout != 'unset' and r3_code_check.stdout != ''
      ignore_errors: true
      
    - name: Check Remote.it jumpbox status (only if registration code set)
      raw: "ps aux | grep -v grep | grep -i remote || echo 'Remote.it processes starting up...'"
      when: r3_code_check.stdout is defined and r3_code_check.stdout != 'unset' and r3_code_check.stdout != ''
      ignore_errors: true
      
    - name: Log Remote.it status (no registration code)
      raw: "echo 'Remote.it jumpbox container created but not configured (no R3_REGISTRATION_CODE provided)'"
      when: r3_code_check.stdout is not defined or r3_code_check.stdout == 'unset' or r3_code_check.stdout == ''